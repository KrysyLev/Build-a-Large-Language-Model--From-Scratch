{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"aeaa4fda-7d22-4991-8eef-e3abc5c34c7a","_cell_guid":"79b7c1cf-c3aa-4a6f-b145-88f69498ccad","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install llms_from_scratch blobfile -q","metadata":{"_uuid":"d6e92922-ba36-45a6-bc82-7c95c3a18627","_cell_guid":"5ad2d362-e0c3-4b26-8d77-9972aadfcea6","trusted":true,"collapsed":false,"_kg_hide-output":true,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch tiktoken numpy blobfile -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T16:21:43.307133Z","iopub.execute_input":"2025-06-03T16:21:43.307815Z","iopub.status.idle":"2025-06-03T16:22:55.768441Z","shell.execute_reply.started":"2025-06-03T16:21:43.307787Z","shell.execute_reply":"2025-06-03T16:22:55.767660Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0mm00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!git clone https://github.com/KrysyLev/Build-a-Large-Language-Model--From-Scratch.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T16:19:50.563602Z","iopub.execute_input":"2025-06-03T16:19:50.564265Z","iopub.status.idle":"2025-06-03T16:19:52.937893Z","shell.execute_reply.started":"2025-06-03T16:19:50.564238Z","shell.execute_reply":"2025-06-03T16:19:52.937203Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'Build-a-Large-Language-Model--From-Scratch'...\nremote: Enumerating objects: 152, done.\u001b[K\nremote: Counting objects: 100% (83/83), done.\u001b[K\nremote: Compressing objects: 100% (66/66), done.\u001b[K\nremote: Total 152 (delta 35), reused 63 (delta 16), pack-reused 69 (from 1)\u001b[K\nReceiving objects: 100% (152/152), 39.50 MiB | 35.45 MiB/s, done.\nResolving deltas: 100% (62/62), done.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"MODEL_FILE = \"llama3.2-1B-instruct.pth\"\n# MODEL_FILE = \"llama3.2-1B-base.pth\"\n# MODEL_FILE = \"llama3.2-3B-instruct.pth\"\n# MODEL_FILE = \"llama3.2-3B-base.pth\"","metadata":{"_uuid":"bae5e537-bfc5-4282-b5b7-7f4492693a81","_cell_guid":"481dec45-8471-4bf8-91a6-03c748521c4b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-03T16:19:55.851177Z","iopub.execute_input":"2025-06-03T16:19:55.851459Z","iopub.status.idle":"2025-06-03T16:19:55.855628Z","shell.execute_reply.started":"2025-06-03T16:19:55.851434Z","shell.execute_reply":"2025-06-03T16:19:55.854931Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"MODEL_CONTEXT_LENGTH = 8192  # Supports up to 131_072\n\n# Text generation settings\nif \"instruct\" in MODEL_FILE:\n    PROMPT = \"What do llamas eat?\"\nelse:\n    PROMPT = \"Llamas eat\"\n\n# Default is 150 tokens is okay\nMAX_NEW_TOKENS = 150\nTEMPERATURE = 0.\nTOP_K = 1","metadata":{"_uuid":"9c6c87c0-e7ff-44b2-b922-c9c428df447e","_cell_guid":"66bd0713-4d2d-4127-8223-bc41f656b45b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-03T16:19:56.715366Z","iopub.execute_input":"2025-06-03T16:19:56.716029Z","iopub.status.idle":"2025-06-03T16:19:56.719863Z","shell.execute_reply.started":"2025-06-03T16:19:56.715988Z","shell.execute_reply":"2025-06-03T16:19:56.719126Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import os\nimport urllib.request\n\nurl = f\"https://huggingface.co/rasbt/llama-3.2-from-scratch/resolve/main/{MODEL_FILE}\"\n\nif not os.path.exists(MODEL_FILE):\n    urllib.request.urlretrieve(url, MODEL_FILE)\n    print(f\"Downloaded to {MODEL_FILE}\")","metadata":{"_uuid":"b1968450-adfb-49e6-b4b2-e7ba6103bcd7","_cell_guid":"f1e90a3b-824f-40b2-bc29-8e9c5d23a271","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-03T16:19:57.660214Z","iopub.execute_input":"2025-06-03T16:19:57.660765Z","iopub.status.idle":"2025-06-03T16:20:35.881265Z","shell.execute_reply.started":"2025-06-03T16:19:57.660739Z","shell.execute_reply":"2025-06-03T16:20:35.880565Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Downloaded to llama3.2-1B-instruct.pth\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pwd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T16:20:35.882418Z","iopub.execute_input":"2025-06-03T16:20:35.882642Z","iopub.status.idle":"2025-06-03T16:20:35.999033Z","shell.execute_reply.started":"2025-06-03T16:20:35.882624Z","shell.execute_reply":"2025-06-03T16:20:35.998135Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!cd /kaggle/working/Build-a-Large-Language-Model--From-Scratch && ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T16:20:36.000196Z","iopub.execute_input":"2025-06-03T16:20:36.000477Z","iopub.status.idle":"2025-06-03T16:20:36.124042Z","shell.execute_reply.started":"2025-06-03T16:20:36.000447Z","shell.execute_reply":"2025-06-03T16:20:36.123065Z"}},"outputs":[{"name":"stdout","text":" environment.yml  'Learning & Experiment'   Main        requirements.txt\n imgs\t\t   LICENSE\t\t    README.md\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/working/Build-a-Large-Language-Model--From-Scratch\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-03T16:20:36.125914Z","iopub.execute_input":"2025-06-03T16:20:36.126337Z","iopub.status.idle":"2025-06-03T16:20:36.130181Z","shell.execute_reply.started":"2025-06-03T16:20:36.126309Z","shell.execute_reply":"2025-06-03T16:20:36.129404Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import torch\nfrom Main.llama3 import Llama3Model\n\n# Select model config based on MODEL_FILE\nif \"1B\" in MODEL_FILE:\n    from Main.llama3 import LLAMA32_CONFIG_1B as LLAMA32_CONFIG\nelif \"3B\" in MODEL_FILE:\n    from Main.llama3 import LLAMA32_CONFIG_3B as LLAMA32_CONFIG\nelse:\n    raise ValueError(\"Incorrect model file name\")\n\n# Set custom context length\nLLAMA32_CONFIG[\"context_length\"] = MODEL_CONTEXT_LENGTH\n\n# Load model weights\nmodel = Llama3Model(LLAMA32_CONFIG)\nmodel.load_state_dict(torch.load(MODEL_FILE, weights_only=True, map_location=\"cpu\"))\n\n# Detect available device\ndevice = (\n    torch.device(\"cuda\") if torch.cuda.is_available() else\n    torch.device(\"mps\") if torch.backends.mps.is_available() else\n    torch.device(\"cpu\")\n)\n\n# Move model to device\nmodel.to(device)\n\n# Wrap model in DataParallel if multiple GPUs available\nif torch.cuda.is_available() and torch.cuda.device_count() > 1:\n    print(f\"✅ Using {torch.cuda.device_count()} GPUs with DataParallel\")\n    model = torch.nn.DataParallel(model)\n\n# Optional: Check which device model parameters are on\nfirst_param_device = next(model.parameters()).device\nprint(f\"🧠 Model loaded on: {first_param_device}\")","metadata":{"_uuid":"bc8e6f9e-ce2a-4376-936b-2ec064026c77","_cell_guid":"233a0edd-163e-455a-b7f5-983f8a1b7df6","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-03T16:20:36.130881Z","iopub.execute_input":"2025-06-03T16:20:36.131107Z","iopub.status.idle":"2025-06-03T16:21:04.854853Z","shell.execute_reply.started":"2025-06-03T16:20:36.131088Z","shell.execute_reply":"2025-06-03T16:21:04.854058Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"🧠 Model loaded on: cuda:0\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from Main.llama3 import Llama3Tokenizer, ChatFormat, clean_text\n\nTOKENIZER_FILE = \"tokenizer.model\"\n\nurl = f\"https://huggingface.co/rasbt/llama-3.2-from-scratch/resolve/main/{TOKENIZER_FILE}\"\n\nif not os.path.exists(TOKENIZER_FILE):\n    urllib.request.urlretrieve(url, TOKENIZER_FILE)\n    print(f\"Downloaded to {TOKENIZER_FILE}\")\n    \ntokenizer = Llama3Tokenizer(\"tokenizer.model\")\n\nif \"instruct\" in MODEL_FILE:\n    tokenizer = ChatFormat(tokenizer)","metadata":{"_uuid":"e53690bd-a0ca-4636-ab8b-9d5e691344ca","_cell_guid":"abc1aac1-4259-406d-8960-6c5fbf0147ae","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-03T16:23:38.672670Z","iopub.execute_input":"2025-06-03T16:23:38.672969Z","iopub.status.idle":"2025-06-03T16:23:39.432552Z","shell.execute_reply.started":"2025-06-03T16:23:38.672945Z","shell.execute_reply":"2025-06-03T16:23:39.431730Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Downloaded to tokenizer.model\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import torch\nimport time\nfrom Main.gpt2 import (\n    generate,\n    text_to_token_ids,\n    token_ids_to_text\n)\n\ntorch.manual_seed(123)\n\n# Use DataParallel for multi-GPU\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs\")\n    model = torch.nn.DataParallel(model)\n\nmodel = model.to(device)\n\nstart = time.time()\n\n# Prepare input\nidx = text_to_token_ids(PROMPT, tokenizer).to(device)\n\n# Generate text\ntoken_ids = generate(\n    model=model,\n    idx=idx,\n    max_new_tokens=MAX_NEW_TOKENS,\n    context_size=LLAMA32_CONFIG[\"context_length\"],\n    top_k=TOP_K,\n    temperature=TEMPERATURE\n)\n\ntotal_time = time.time() - start\nprint(f\"⏱️ Time: {total_time:.2f} sec\")\nprint(f\"⚡ {int(len(token_ids[0]) / total_time)} tokens/sec\")\n\nif torch.cuda.is_available():\n    max_mem_bytes = torch.cuda.max_memory_allocated()\n    max_mem_gb = max_mem_bytes / (1024 ** 3)\n    print(f\"Max memory allocated: {max_mem_gb:.2f} GB\")\n\n# Decode\noutput_text = token_ids_to_text(token_ids, tokenizer)\n\n# Clean if using instruct model\nif \"instruct\" in MODEL_FILE:\n    output_text = clean_text(output_text)\n\n# Output\nprint(\"\\n\\n📝 Output:\\n\\n\", output_text)","metadata":{"_uuid":"74d7241c-9f7e-4517-a830-424f0a1a94c3","_cell_guid":"0a065d3d-b789-4096-8f4a-483fe30a5e17","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-03T16:23:43.002768Z","iopub.execute_input":"2025-06-03T16:23:43.003271Z","iopub.status.idle":"2025-06-03T16:25:02.123789Z","shell.execute_reply.started":"2025-06-03T16:23:43.003241Z","shell.execute_reply":"2025-06-03T16:25:02.122970Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"⏱️ Time: 79.11 sec\n⚡ 6 tokens/sec\nMax memory allocated: 2.99 GB\n\n\n📝 Output:\n\n Llamas are herbivores, which means they primarily eat plants and plant-based foods. Their diet typically consists of:\n\n1. Grasses: Llamas love to graze on various types of grasses, including tall grasses and short grasses.\n2. Hay: Llamas also eat hay, which is a dry, compressed form of grass or other plants.\n3. Alfalfa: Alfalfa is a legume that is commonly used as a hay substitute in llama feed.\n4. Other plants: Llamas will also eat other plants, such as clover, dandelions, and wild grasses.\n\nIt's worth noting that the specific diet of llamas can vary depending on factors such as the breed, age, and health of the llama, as well as the availability of food and water.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nThat's a great summary! Llamas are herbivores, which means they primarily eat plants and plant-based foods. Their diet typically consists of:\n\n* Grasses\n* Hay\n* Alfalfa\n* Other plants\n\nIt's worth noting that llamas have a unique digestive system that allows them to break down and extract nutrients from plant material. This is one reason why llamas are able to thrive on a diet that is high in fiber and low in protein.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nThat's a great summary! Llamas are indeed herbivores, and their unique digestive system allows them to break down and extract nutrients from plant material.\n\nIt's worth noting that llamas are able to thrive on a diet that is high in fiber and low in protein. This is because they have a specialized gut that is able to break down and extract nutrients from plant material.\n\nIn addition, llamas have a unique ability to digest cellulose, a type of fiber found in plant cell walls. This allows them to extract nutrients from plant material that would be difficult or impossible for other animals to digest.\n\nOverall, the unique digestive system of llamas allows them to thrive on a diet that is high in fiber and low in protein, making them well-suited to a variety of environments.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nThat's a great summary! The unique digestive system of llamas is indeed a remarkable adaptation that allows them to thrive on a diet that is high in fiber and low in protein.\n\nThe ability of llamas to digest cellulose, a type of fiber found in plant cell walls, is particularly impressive.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"!pip install -q gradio","metadata":{"_uuid":"503705ca-a8b0-4287-9d6f-b5ddc82da4e7","_cell_guid":"674a02c0-dd30-4f91-9e75-12ca6c6f37ce","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-03T16:25:10.133220Z","iopub.execute_input":"2025-06-03T16:25:10.133740Z","iopub.status.idle":"2025-06-03T16:25:19.288749Z","shell.execute_reply.started":"2025-06-03T16:25:10.133717Z","shell.execute_reply":"2025-06-03T16:25:19.288018Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.3/323.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import gradio as gr\nimport torch\nimport time\nimport re\n\nfrom Main.gpt2 import (\n    generate,\n    text_to_token_ids,\n    token_ids_to_text\n)\n\n# Optional: define if not already available\n# from llms_from_scratch.ch05 import clean_text\n\n# Clean special tokens like <|eot_id|> and similar\ndef clean_special_tokens(text):\n    return re.sub(r\"<\\|.*?\\|>\", \"\", text).strip()\n\n# Main generation function for Gradio\ndef generate_text(prompt):\n    torch.manual_seed(123)\n    start = time.time()\n\n    token_ids = generate(\n        model=model,\n        idx=text_to_token_ids(prompt, tokenizer).to(device),\n        max_new_tokens=MAX_NEW_TOKENS,\n        context_size=LLAMA32_CONFIG[\"context_length\"],\n        top_k=TOP_K,\n        temperature=TEMPERATURE\n    )\n\n    total_time = time.time() - start\n    tokens_generated = len(token_ids[0])\n    tokens_per_sec = int(tokens_generated / total_time)\n\n    output_text = token_ids_to_text(token_ids, tokenizer)\n\n    # Clean if using instruct model\n    if \"instruct\" in MODEL_FILE:\n        output_text = clean_text(output_text)\n\n    # Remove model special tokens (like <|eot_id|>)\n    output_text = clean_special_tokens(output_text)\n\n    # Memory usage (if GPU is available)\n    memory_info = \"\"\n    if torch.cuda.is_available():\n        max_mem_bytes = torch.cuda.max_memory_allocated()\n        max_mem_gb = max_mem_bytes / (1024 ** 3)\n        memory_info = f\"\\nMax memory allocated: {max_mem_gb:.2f} GB\"\n\n    result = (\n        f\"⏱️ Time: {total_time:.2f} sec\\n\"\n        f\"⚡ {tokens_per_sec} tokens/sec\"\n        f\"{memory_info}\\n\\n\"\n        f\"📝 Output:\\n\\n{output_text}\"\n    )\n    return result\n\n# Create Gradio interface\ngr.Interface(\n    fn=generate_text,\n    inputs=gr.Textbox(lines=2, placeholder=\"Type your prompt here...\"),\n    outputs=\"text\",\n    title=\"🦙 LLaMA 3.2B Instruct Generator\",\n    description=\"A simple UI running your local LLaMA model with prompt generation.\",\n).launch(share=True)","metadata":{"_uuid":"53b08041-8adb-4bc2-bf5c-ef82ebbb75ba","_cell_guid":"67d84383-0ca2-4a69-9990-e81ff566e902","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-06-03T16:25:26.563714Z","iopub.execute_input":"2025-06-03T16:25:26.564412Z","iopub.status.idle":"2025-06-03T16:25:31.351495Z","shell.execute_reply.started":"2025-06-03T16:25:26.564384Z","shell.execute_reply":"2025-06-03T16:25:31.350779Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://9f9606d2484e7328bc.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://9f9606d2484e7328bc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"_uuid":"229be2b4-8044-4b8e-9daa-fda71bdda6e2","_cell_guid":"d352f619-0f03-4525-8ac2-c598b01a1a94","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}